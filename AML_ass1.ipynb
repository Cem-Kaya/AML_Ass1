{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning - programming assignment 1\n",
    "\n",
    "*Due: Friday November 29th*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please fill in:**\n",
    "* Cem Kaya (9276866)\n",
    "* Gaynora van Dommelen (6717659)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further instructions:\n",
    "* Code quality is considered during the assessment. Make sure your code is properly commented. \n",
    "* Submit your code in Blackboard using **one** of your accounts; we will put the grade in Blackboard for the other team member as well.\n",
    "* Make sure to name the submitted file according to your and your collaborators last name (i.e. submitter_collaborator.ipynb). \n",
    "* **Failure to follow these instructions can affect the assignment grade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Please check the given 'README' file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soq1skOjrr6z"
   },
   "source": [
    "### 1. Let's start with the OpenAI gym\n",
    "\n",
    "Gym/Gymnasium (https://gymnasium.farama.org/) is a wide-used toolkit for developing and comparing reinforcement learning algorithms. \n",
    "\n",
    "1. Gym/Gymnasium makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano. \n",
    "\n",
    "2. The library is a collection of test problems — **environments** — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "on5JDYmWaK-w"
   },
   "source": [
    "**Great!** Now let's import the gymnasium class and work on a basic example of gym code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hK-jC9ceDuY"
   },
   "outputs": [],
   "source": [
    "import gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLePIoe7VJXS"
   },
   "source": [
    "Like mentioned above, gymnasium's main purpose is to provide a large collection of **environments** that expose a common interface. You can find a listing of those environments below (they are Markov decision process(MDP) environments and we discussed MDP in our lecture 2), as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import envs\n",
    "print(envs.registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to explain how the RL framework of gym works. \n",
    "- An **ENVIRONMENT**, \n",
    "- You also have an **AGENT**,\n",
    "- The agent takes an **ACTION**, in our case, 10 actions are possible to take,\n",
    "- When a single **ACTION** is chosen and fed to our **ENVIRONMENT**, the **ENVIRONMENT** measures how good the action was taken and produces a **REWARD**, which is usually a numeric value.\n",
    "\n",
    "In MDP problems, the **ENVIRONMENT** will also provides an **OBSERVATION**, which represets the state of the **ENVIRONMENT** at the current moment. In the multi-armed bandit problems, there is no **OBSERVATION** (or state) or one constant state. \n",
    "\n",
    "Please read the 'Basic usage' https://gymnasium.farama.org/introduction/basic_usage/ for better understanding the framework. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA-998XefX85"
   },
   "source": [
    "### 2. Implement your own environment\n",
    "\n",
    "Next, we are going to implement our own environment following the framework of gym. This enviroment is a gambiling room with ten different slot machines (a 10-armed bandit problem). Similar with examples given in the lecture, the reward of each slot machine follows a normal distribution, but the average reward (mean) and variance of each action are different. Your goal is to determine the optimal action from all possible actions/machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core gym interface is **Env**, which is the unified environment interface. There is no interface for agents. The following are the Env methods you should know:\n",
    "\n",
    "- `step(self, action)`: Steps the environment by one timestep. Returns observation, reward, done, info.\n",
    "- `reset(self)`: Resets the environment to an initial state. Returns an initial observation. Each call of `reset()` should yield an environment suitable for a new episode, independent of previous episodes. Because there is no state transition in multi-armed bandit problems, this function is not used here.\n",
    "- `render(self, mode='human')`: Renders one frame of the environment. The default mode will do something human friendly, such as pop up a window. In this assignment, there is no need to create a pop up window. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before writing your own codes, read through the readme of github page of gymasium (https://github.com/Farama-Foundation/Gymnasium). You are also recommended to read at least the codes for one simple environment and one example agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Self-defined Slot Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please fill in the missing codes in the function sample (1 point).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGDaa_u8fjO3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class slotMachine:\n",
    "    \"\"\"\n",
    "        A slot machine contains a reward distribution that randomly generated with restricted mean and standard deviation. \n",
    "            sample function: generates a reward at each time step based on the given reward distribition\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.mu = np.random.uniform(-5, 5)  # mean\n",
    "        self.sigma = np.random.uniform(0.5, 1)  # standard deviation\n",
    "\n",
    "    def sample(self):\n",
    "        ########## TODO: to be filled. ########## \n",
    "        return np.random.normal(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Game Environment\n",
    "**Please fill in the missing codes in function step (1 point) in the environment.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "# The environment has to inherit the interface of gymnasium.Env\n",
    "class GamblingRoom(gymnasium.Env):\n",
    "    \"\"\"\n",
    "    A k-armed bandit environment: a gambling room with slot machines, allows the agents to interact with it.\n",
    "        r_machines: A list of slot machines, each gamblingRoom contains k number of slotMachines\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, seed: int=None):\n",
    "        # set random seed\n",
    "        self.seed(seed)     \n",
    "        # initialize reward distribution for each action/machine\n",
    "        self.r_machines: List[slotMachine] = []\n",
    "        for i in range(k):\n",
    "            # each gamblingRoom contains k number of slotMachines\n",
    "            self.r_machines.append(slotMachine())\n",
    "\n",
    "        self.num_arms: int = k\n",
    "        self.action_space = spaces.Discrete(self.num_arms)\n",
    "        self.observation_space = spaces.Discrete(1)\n",
    "        # for our bandit environment, the state is constant\n",
    "        self.state: int = 0\n",
    "    \n",
    "    # step up the environment based on the selected action,\n",
    "    # return the constant state, reward, done = false, and info \n",
    "    # for now, we do not have to worry about the DONE and INFO variables.\n",
    "    def step(self, action: int) -> Tuple[int, float, bool, dict]:\n",
    "        assert self.action_space.contains(action)\n",
    "        done = False\n",
    "\n",
    "        ########## TODO: to be filled. ##########\n",
    "        reward = self.r_machines[action].sample()\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    # random seed used for reproducibility purposes\n",
    "    def seed(self, seed: int):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "    def render(self, mode:str='human', close:bool=False):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QFSX4fjxrh8"
   },
   "source": [
    "### 3. Implement an agent with the epsilon greedy algorithm\n",
    "\n",
    "In this part, you are expected to implement an RL agent. To decide the action to take at each time step, this agent uses the epsilon greedy algorithm introduced in the lecture.\n",
    "\n",
    "**Please fill in the missing codes in function select_action (1.5 points) and update_parameters (1 point) in the agent.** Feel free to import the needed packages if there are any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NWI9R9BiybZl"
   },
   "outputs": [],
   "source": [
    "class EplisonGreedyAgent:\n",
    "    def __init__(self, k: int, e: float):\n",
    "        # set up the number of arms/actions\n",
    "        self.num_arms: int = k\n",
    "        # set up the value of epsilon\n",
    "        self.epsilon: float = e\n",
    "        # init the estimated values of all actions\n",
    "        self.Qvalues: np.ndarray = np.zeros(k)\n",
    "        # init the numbers of time step that every action is selected\n",
    "        self.stepSize: np.ndarray = np.zeros(k)\n",
    "\n",
    "    ##\n",
    "    # select the action to take at the current time step\n",
    "    # (for MDP, choose the action based on state; for k-armed bandit, no state given)\n",
    "    # return: the action to take\n",
    "    ##\n",
    "    def select_action(self) -> int:\n",
    "        \"\"\"\n",
    "        Select the action to take at the current time step using the epsilon greedy algorithm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The index of the selected action\n",
    "        \"\"\"\n",
    "        ########## TODO: to be filled. ##########   \n",
    "        if np.random.rand() < self.epsilon: # exploration\n",
    "            return np.random.randint(0, self.num_arms)\n",
    "        else: # exploitation\n",
    "            return np.argmax(self.Qvalues) # return index of largest Qvalue\n",
    "\n",
    "    ##\n",
    "    # Update the Q-values of the agent based on received rewards\n",
    "    # input: action_index = the action, reward = the reward from this action\n",
    "    # return: null\n",
    "    ##\n",
    "    def update_parameters(self, action: int, reward: float) -> None:\n",
    "        \"\"\"\n",
    "        Update the number of times an action was taken as well as the running average of the Q-values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action: int\n",
    "            The index of the action which needs to be updated\n",
    "        reward: float\n",
    "            The reward corresponding to the provided action\n",
    "        \"\"\"\n",
    "        ########## TODO: to be filled. ##########  \n",
    "        self.stepSize[action] += 1\n",
    "        self.Qvalues[action] += (reward - self.Qvalues[action]) / self.stepSize[action]\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me9kN1MPWEEf"
   },
   "source": [
    "### 4. Run the simulation, play with parameters and analyse results\n",
    "\n",
    "Finally, we write codes for running the simulation. \n",
    "\n",
    "In order to decrease the effect of randomness, we usually conduct multiple simulation runs and average the results. In the implementation, you may start with one run, then use the variable `num_runs` for running multiple simulations.\n",
    "\n",
    "In each run, you shall setup the `epsilon` and number of time step `num_episodes` (0.01 and 500 by default). Then, after the initlization of our agent and environment, **please fill in the missing codes (with ??? or TODO: to be filled). (2.5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tested hyper parameters\n",
    "num_episodes_list = [1000, 2000, 5000]\n",
    "epsilon_list = [0, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(k: int, seed: int, num_runs: int, num_episodes: int, epsilon: float) -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"Simulate multiple runs of the k-bandit problem depicted as a gambling room\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    k: int\n",
    "        The number of arms (i.e., slot machines)\n",
    "    seed: int\n",
    "        Used for reproducibility\n",
    "    num_runs: int\n",
    "        The number of iterations the simulation must go through\n",
    "    num_episodes: int\n",
    "        The number of steps that the agent must take during each run\n",
    "    epsilon: float\n",
    "        The fraction of greediness of the agent\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[float], List[float]]\n",
    "        List of average rewards and list of optimal actions counts\n",
    "    \"\"\"\n",
    "    # init the environment and set up the random seed\n",
    "    env = GamblingRoom(k=k, seed=seed)\n",
    "\n",
    "    # delete the wrap\n",
    "    env = env.unwrapped\n",
    "\n",
    "    # show the action space\n",
    "    print(env.action_space)\n",
    "\n",
    "    # to store average reward at each timestep\n",
    "    average_rewards = np.zeros(num_episodes)\n",
    "    # to store the amount of times the optimal action has been chosen\n",
    "    optimal_action_counts = np.zeros(num_episodes)\n",
    "\n",
    "    # run simulations\n",
    "    for i_run in range(num_runs):\n",
    "        agent = EplisonGreedyAgent(k=k, e=epsilon)\n",
    "\n",
    "        true_action_values = np.array([machine.mu for machine in env.r_machines])\n",
    "        optimal_action = np.argmax(true_action_values)\n",
    "        \n",
    "        for t in range(num_episodes):\n",
    "            action = agent.select_action() # make the agent choose its next action\n",
    "            \n",
    "            _, reward, _, _ = env.step(action) # make the agent take a step\n",
    "            \n",
    "            agent.update_parameters(action, reward) # update the params of the agent based on the achieved rewards\n",
    "             \n",
    "            average_rewards[t] += reward # store reward\n",
    "            \n",
    "            if action == optimal_action: # if the optimal action was chosen\n",
    "                optimal_action_counts[t] += 1 # increment the counter for optimal action at time step t\n",
    "            \n",
    "    env.close()\n",
    "    average_rewards /= num_runs\n",
    "    optimal_action_counts /= num_runs\n",
    "    return average_rewards, optimal_action_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage of results for each huper parameter\n",
    "results = {}\n",
    "\n",
    "for num_episodes in num_episodes_list:\n",
    "    print(f\"Running for {num_episodes} episodes\")\n",
    "    for epsilon in epsilon_list:\n",
    "        \n",
    "        num_action = 10\n",
    "        num_seed = 5\n",
    "        num_runs = 2000  # number of simulation runs\n",
    "        average_rewards, optimal_action_counts = simulate(k=num_action, seed=num_seed, num_runs=num_runs, num_episodes=num_episodes, epsilon=epsilon)\n",
    "        \n",
    "        # Store the results for this combination of (num_episodes, epsilon)\n",
    "        results[(num_episodes, epsilon)] = {\n",
    "            \"average_rewards\": average_rewards,\n",
    "            \"optimal_action_counts\": optimal_action_counts,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simulation_evaluation(results: Dict[Tuple[int, float], Dict[str, List[float]]]) -> plt.Axes:\n",
    "    \"\"\"Plot simulation evaluation for agents with different epsilon values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    results: Dict[Tuple[int, float], Dict[str, List[float]]]\n",
    "        Key is a tuple of the number of episodes and epsilon with values:\n",
    "        - 'average_rewards': List of average rewards for each episode\n",
    "        - 'optimal_action_counts': List of counters for the times an optimal action was taken\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Axes\n",
    "        Axes objects of the average rewards over episodes and the percentage of optimal actions chosen over episodes\n",
    "    \"\"\"\n",
    "    # Create figure to store plots\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "    \n",
    "    for (episodes, epsilon), data in results.items():\n",
    "        # Plot the average rewards\n",
    "        ax[0].plot(\n",
    "            range(episodes),\n",
    "            data['average_rewards'],\n",
    "            label=f\"$\\\\varepsilon$={epsilon}\"\n",
    "        )\n",
    "        # Plot the percentage of optimally chosen actions\n",
    "        ax[1].plot(\n",
    "            range(episodes),\n",
    "            data['optimal_action_counts']*100,\n",
    "            label=f\"$\\\\varepsilon$={epsilon}\"\n",
    "        )\n",
    "    # Style the graph of the average reward performance\n",
    "    ax[0].set_title(\"Average reward performance\")\n",
    "    ax[0].set_xlabel(\"Episodes\")\n",
    "    ax[0].set_ylabel(\"Average reward\")\n",
    "    ax[0].legend(loc=\"lower right\")\n",
    "    # Style the graph of the percentage optimally chosen actions\n",
    "    ax[1].set_title(\"Percentage optimally chosen actions\")\n",
    "    ax[1].set_xlabel(\"Episodes\")\n",
    "    ax[1].set_ylabel(\"% Optimal action\")\n",
    "    ax[1].legend(loc=\"lower right\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results_episode = lambda n_episodes: dict((k,v) for k,v in results.items() if k in list(filter(lambda key: n_episodes == key[0], results.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_simulation_evaluation(get_results_episode(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_simulation_evaluation(get_results_episode(2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_simulation_evaluation(get_results_episode(5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgJlSiCGbMBP"
   },
   "source": [
    "Now it's time to examine the performance of algorithms with different epsilon values (different exploration strategies) in multiple simulation runs. \n",
    "\n",
    "You shall play with the parameter epsilon under 2 or 3 different gambling environments (by initlizing different reward distributions for machines). **For each environment, try at least 2 different values of epsilon and identify a reasonable epsilon value that could balance the exploration and exploiation**. **Instead of handing in your codes for this part, please select one environment you have tested and describe your environment and experimental settings (1 point)**. Then, provide an explanation on how you identify the good epsilon value in this environment and why it is a good one **(1 point)**. \n",
    "\n",
    "Few instructions:\n",
    "- Your answer shall include two plots presenting compariable measures of the different epsilon settings (e.g. the average reward per step and % of optimal action). **(1 point)** \n",
    "- You shall present the average results from at least 100 simulation runs. Remember that the gambling environment CANNOT be changed over those runs used for calculating the average results. \n",
    "- You may adjust the total time steps when the learning needs more time for a cerain epsilon value, but do not over spend your time on this.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Put your answer (at most 300 words) with accompanying plots here.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **CHOSEN ENVIRONMENT AND EXPERIMENTAL SETTINGS**  \n",
    "> * `num_runs=2000`, to limit the oscillation and variation between episodes due to the number of runs, the number of runs was adjusted to `2000`; copying the number of runs used in the RL book to demonstrate the performance of $\\varepsilon$-greediness.\n",
    "> * `num_episodes_list = [1000, 2000, 5000]`, within the book they mention that an agent using $\\varepsilon=0.01$ greediness performs better in the long run than that of $\\varepsilon=0.1$. This was, however, not depicted within the graph in the RL book. Therefore, additional number of episodes were chosen to see when the $\\varepsilon=0.01$ strategy overtakes $\\varepsilon=0.1$, if it happens at all.\n",
    "> * `epsilon_list = [0, 0.01, 0.1]`, again using the RL book as inspiration, the epsilon values include a fully greedy one exploring to its hearts content and less greedy ones.\n",
    ">\n",
    "> **OPTIMAL PERFORMING EPSILON AGENT**  \n",
    "> As determined within the RL book, the results show that an epsilon-greedy agent with $\\varepsilon=0.01$ performs better in the long run compared to $\\varepsilon=0.1$. This can be seen when the number of episodes is increased to above $1000$. Then between $2000$ and $3000$ episodes the average reward seems to have reached its optimal and evens out, while the optimally chosen actions by the agent with $\\varepsilon=0.01$ only reaches its optimum near $5000$ episodes; effectively overtaking the percentage of optimally chosen actions when an agent uses $\\varepsilon=0.1$. Therefore, the best performing epsilon agent uses a $\\varepsilon=0.01$ to determine its greediness.\n",
    ">\n",
    "> ![image.png](img/epsilon-greedy-agent-episodes-5000.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are almost done! Before handing in, make sure that the codes you hand in work, and that all plots are shown. **Submit just one file per team.** Please make sure that you submit a .zip file with images.\n",
    "\n",
    "Again, make sure you name this file according to your last names."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lesson 1: Multi- Armed Bandit with OpenAi Gym ver 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "AML_RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
